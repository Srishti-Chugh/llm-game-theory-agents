# ğŸ§  LLM Agents in Repeated Strategic Games: A Game-Theoretic and Entropy-Based Study

## ğŸ“Œ Project Overview

This project investigates the behavior of Large Language Model (LLM) agents in repeated strategic games using concepts from **Game Theory**, **Information Theory (Entropy)**, and **Multi-Agent Systems**.

We study how LLM agents:

* Choose actions in repeated Prisonerâ€™s Dilemma and Trust Games
* Converge to Nash Equilibrium strategies
* Exhibit **entropy collapse** (loss of strategic diversity)
* Respond to different **linguistic framings** (neutral vs moral prompts)

The project aims to provide empirical and theoretical insights into:

> How language-based reasoning systems behave as rational agents in strategic environments.

---

## ğŸ¯ Motivation & Novelty

While prior work benchmarks LLMs on games, this project introduces:

* âœ… **Entropy-based analysis** of LLM strategies over time
* âœ… Study of **language framing (moral vs neutral)** as a utility modifier
* âœ… Empirical demonstration of **Nash equilibrium convergence**
* âœ… Modular framework for experimenting with LLM-based agents
* âœ… Foundation for future work in Bayesian games and multi-agent orchestration

This bridges:

* Game Theory
* Human-AI Interaction
* Cognitive Systems
* Multi-Agent Learning

---

## ğŸ§© Key Components

### 1. Games Implemented

* Prisonerâ€™s Dilemma
* Trust Game (extendable)

Each game defines:

* Action space
* Payoff matrix
* History tracking

---

### 2. Agents

* **SimpleAgent / RandomAgent** (baseline)
* **LLMAgent** (uses prompts + game history)

LLM agents:

* Read game history
* Reason using prompt templates
* Output structured actions (C or D)

---

### 3. Metrics

Implemented in `metrics/`:

* **Action Entropy** (strategy randomness)
* Cooperation rate
* Payoff tracking
* Round-by-round logging

Entropy is used to detect:

> Strategic collapse vs sustained diversity.

---

## ğŸ“ Project Structure

```
llm-game-theory-agents/
â”‚
â”œâ”€â”€ games/
â”‚   â”œâ”€â”€ prisoners_dilemma.py
â”‚   â”œâ”€â”€ trust_game.py
â”‚
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ llm_agent.py
â”‚
â”œâ”€â”€ prompts/
â”‚   â”œâ”€â”€ neutral.txt
â”‚   â”œâ”€â”€ moral.txt
â”‚
â”œâ”€â”€ experiments/
â”‚   â””â”€â”€ run_two_agent_game.py
â”‚
â”œâ”€â”€ metrics/
â”‚   â”œâ”€â”€ entropy.py
â”‚   â”œâ”€â”€ logger.py
â”‚
â”œâ”€â”€ plots/
â”‚   â””â”€â”€ plot_results.py
â”‚
â”œâ”€â”€ results/
â”‚
â”œâ”€â”€ config.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore
```

---

## âš™ï¸ Setup Instructions

### 1. Clone Repository

```bash
git clone https://github.com/your-username/llm-game-theory-agents.git
cd llm-game-theory-agents
```

### 2. Create Virtual Environment

```bash
python -m venv venv
source venv/bin/activate   # Mac/Linux
venv\Scripts\activate      # Windows
```

### 3. Install Requirements

```bash
pip install -r requirements.txt
```

### 4. Add API Key

Create `.env` file:

```
OPENAI_API_KEY=your_api_key_here
```

---

## â–¶ï¸ Running Experiments

Run a two-agent Prisonerâ€™s Dilemma experiment:

```bash
python experiments/run_two_agent_game.py
```

This will:

* Run repeated game rounds
* Log actions and payoffs
* Compute entropy for each agent
* Save results to `results/`

---

## ğŸ“Š Example Output

Sample result:

```
Round, Action_A, Action_B, Payoff_A, Payoff_B
1, D, C, 5, 0
2, D, D, 1, 1
...
Entropy Agent A: 0.0
Entropy Agent B: 0.46
```

Interpretation:

* LLM agents converge to defection (Nash equilibrium)
* Entropy collapses â†’ deterministic strategy
* Moral framing can alter cooperation levels

---

## ğŸ§ª Experiments Supported

* LLM vs LLM
* LLM vs Random Agent
* Neutral vs Moral framing
* Multiple runs with entropy analysis
* Strategy convergence tracking

---

## ğŸ”¬ Research Questions

This project explores:

1. Do LLM agents converge to Nash Equilibrium in repeated games?
2. Does linguistic framing influence cooperation?
3. How fast does entropy collapse?
4. Are LLM strategies stable across runs?
5. How does reasoning differ from payoff maximization?

---

## ğŸš€ Future Extensions

Planned enhancements:

* Bayesian incomplete-information games
* Multi-agent societies (3+ agents)
* Entropy dynamics over long horizons
* Test-time scaling (longer reasoning chains)
* Evolutionary agent populations
* Visualization dashboards

---

## ğŸ§  Scientific Relevance

This project contributes to:

* Game Theory in AI
* Cognitive Systems & Human-AI Interaction
* Multi-Agent Systems
* Empirical analysis of LLM reasoning
* Strategic alignment research

---
